# TouchÂ inÂ theÂ Wild: Learning Fineâ€‘Grained Manipulation with a Portable Visuoâ€‘Tactile Gripper

[\[ProjectÂ page\]](https://binghao-huang.github.io/touch_in_the_wild/) Â· [\[Paper\]](https://binghao-huang.github.io/touch_in_the_wild/) Â· [\[ROS2 & Data Collection Tutorial\]](https://github.com/YolandaXinyueZhu/touch_in_the_wild-data-collection) 

<img width="90%" src="assets/teaser.png"></p>

[Xinyue Zhu](https://binghao-huang.github.io/touch_in_the_wild/)<sup>\* 1</sup>,
[Binghao Huang](https://binghao-huang.github.io/)<sup>\* 1</sup>,
[Yunzhu Li](https://yunzhuli.github.io/)<sup>1</sup>

<sup>\*</sup>Equal contribution <sup>1</sup>Columbia University


## ğŸ› ï¸Â Installation

> **Tested on UbuntuÂ 22.04**

1. **System dependencies & Docker**
   Follow the [Universal Manipulation Interface](https://github.com/real-stanford/universal_manipulation_interface) guide to install Docker and all required system packages.
2. **Conda environment**
   We recommend [Miniforge](https://github.com/conda-forge/miniforge) + *mamba* for faster solves.

   ```bash
   mamba env create -f conda_environment.yaml
   mamba activate touchwild
   ```


## ğŸ“Â Creating a Dataset with the SLAM Pipeline

The SLAM pipeline aligns GoPro videos with tactile logs and produces a timeâ€‘synchronised dataset.

1. **Collect data**
   Follow the [Touchâ€‘inâ€‘theâ€‘Wild ROSâ€¯2 Guide](https://github.com/YolandaXinyueZhu/touch_in_the_wild-data-collection) to record GoPro streams **plus** tactile JSON logs.
   For detailed instructions on collecting demonstrations with the UMI gripper, see the [Data Collection Tutorial](https://swanky-sphere-ad1.notion.site/UMI-Data-Collection-Tutorial-4db1a1f0f2aa4a2e84d9742720428b4c?pvs=4).
2. **Organise files**
    Collect **all** videos recorded during the sessionâ€”including

   * **demo videos**
   * **mapping videos**
   * **gripper calibration video**
   
   â€”and the associated tactile JSON file, and place everything in **one folder**:

   ````text
   <YOUR_SESSION_FOLDER>/
   â”œâ”€â”€ demo_mapping.mp4
   â”œâ”€â”€ demo_gripper.mp4
   â”œâ”€â”€ demo_0001.mp4
   â”œâ”€â”€ demo_0002.mp4
   â””â”€â”€ tactile_recording_YYYYMMDD_HHMMSS.json
   ````

3. **Run the pipeline**

   ```bash
   (touchwild)$ python run_slam_pipeline.py <YOUR_SESSION_FOLDER> --bag <YOUR_SESSION_FOLDER>/tactile_recording_YYYYMMDD_HHMMSS.json
   ```
   All SLAM outputs are written back into `<YOUR_SESSION_FOLDER>/`.
   
5. **Generate training dataset**

   ```bash
   (touchwild)$ python scripts_slam_pipeline/07_generate_replay_buffer.py <YOUR_SESSION_FOLDER> -o <YOUR_SESSION_FOLDER>/dataset.zarr.zip
   ```

## ğŸ–ï¸Â Building a *visuo-tactileâ€‘only* Dataset

`run_tactile_pipeline.py` builds a *visuo-tactile* dataset with the same Zarr layout as the full SLAM pipeline, but containing only GoPro and tactile images for selfâ€‘supervised MAE preâ€‘training.

```bash
(touchwild)$ python run_tactile_pipeline.py --bag /path/to/tactile_recording_YYYYMMDD_HHMMSS.json
```

Generate visuo-tactile-only training dataset:

```bash
(touchwild)$ python scripts_tactile_pipeline/04_generate_replay_buffer.py <YOUR_SESSION_FOLDER> -o <YOUR_SESSION_FOLDER>/dataset.zarr.zip
```

## ğŸ§‘â€ğŸ”¬Â Preâ€‘training the Visuoâ€‘Tactile MAE

1. **Dataset**Â â€“ use any `dataset.zarr.zip` created above.
2. **Launch training**

   ```bash
   (touchwild)$ python -m pretrain_mae.pretrain_mae task.dataset_path=/path/to/dataset.zarr.zip
   ```

   Checkpoints are stored in `pretrain_mae/pretrain_checkpoints/`.

### ğŸ”Â Evaluate a checkpoint

We provide an example pretrained MAE checkpoint.

```bash
pip install -U "huggingface_hub[cli]"

huggingface-cli download \
    xinyue-zhu/pretrained_mae \
    pretrain_mae.pth \
    config.yaml \
    --repo-type model \
    --local-dir ./pretrain_checkpoints
```

To evaluate the pretrained checkpoint on the tactile reconstruction task:

```bash
(touchwild)$ python -m pretrain_mae.pretrain_eval --checkpoint /path/to/mae_checkpoint.pth --dataset /path/to/dataset.zarr.zip --plot_images    
```

The script reports Meanâ€‘Squaredâ€‘Error (MSE) on the validation split and, with `--plot_images`, saves qualitative results to `eval_outputs/`.


## ğŸ“ˆÂ Training DiffusionÂ Policies

We provide an example `test_tube_collection` dataset (\~13â€¯GB).

```bash
pip install -U "huggingface_hub[cli]"

huggingface-cli download \
    xinyue-zhu/test_tube_collection \
    test_tube_collection.zarr.zip \
    --repo-type dataset \
    --local-dir ./dataset
```

### Singleâ€‘GPU

```bash
(touchwild)$ python train.py \
  --config-name train_diffusion_unet_timm_umi_workspace \
  task.dataset_path=/path/to/dataset.zarr.zip \
  policy.obs_encoder.use_tactile=true \
  policy.obs_encoder.tactile_model_choice=pretrain \
  policy.obs_encoder.pretrain_ckpt_path=/path/to/mae_checkpoint.pth
```

### Multiâ€‘GPU

```bash
(touchwild)$ accelerate --num_processes <NGPUS> train.py \
  --config-name train_diffusion_unet_timm_umi_workspace \
  task.dataset_path=/path/to/dataset.zarr.zip \
  policy.obs_encoder.use_tactile=true \
  policy.obs_encoder.tactile_model_choice=pretrain \
  policy.obs_encoder.pretrain_ckpt_path=/path/to/mae_checkpoint.pth
```

## ğŸ¦¾Â Realâ€‘World Deployment

Below we demonstrate deploying a trained policy on **xArmÂ 850**.

### ğŸ¥Â Camera Setup

Refer to the [UMI Hardware Guide](https://github.com/real-stanford/universal_manipulation_interface?tab=readme-ov-file#-real-world-deployment) for GoPro configuration.

### ğŸ–ï¸Â Tactile Setup

1. Physically connect both tactile sensors to the machine running the policy.
2. Follow the [tactile hardware guide](https://github.com/YolandaXinyueZhu/TactileUMI_ros2/tree/main?tab=readme-ov-file#-tactile-hardware) to configure persistent port naming.

### ğŸ¤– RobotÂ Setup

#### Install the xArm Python SDK

```bash
# From outside the repository
(touchwild)$ cd ..
(touchwild)$ git clone https://github.com/xArm-Developer/xArm-Python-SDK.git
(touchwild)$ cd xArm-Python-SDK
(touchwild)$ pip install .
```

#### Launch uFactory Studio

1. Download **UFactoryStudioâ€‘Linuxâ€‘1.0.1.AppImage** from the [uFactory website](https://www.ufactory.us/ufactory-studio).
3. Connect to the robot's IP address.
4. Go to Settings â†’ Motion â†’ TCP and set the payload to:

   * **Weight:** 1.9â€¯kg
   * **Center of Mass (CoM):** x = -2â€¯mm, y = -6â€¯mm, z = 37â€¯mm

5. Go to Settings â†’ Motion â†’ TCP and set the TCP offset to: `(x = 0â€¯mm, y = 0â€¯mm, z = 270â€¯mm, roll = 0Â°, pitch = 0Â°, yaw = 90Â°)`

#### Configure IP

Edit the configuration file to set the robot's IP address:

```yaml
# File: /example/eval_robots_config.yaml
robot_ip: <your_robot_ip_here>
```

### â˜•Â Running a Policy

```bash
# Allow access to the HDMI capture card
sudo chmod -R 777 /dev/bus/usb

# Evaluate a checkpoint
(touchwild)$ python eval_real.py --robot_config example/eval_robots_config.yaml -i /path/to/policy_checkpoint.ckpt -o /path/to/output_folder

```
## ğŸ–ï¸Â Related Works

3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing. [link](https://binghao-huang.github.io/3D-ViTac/).

VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning. [link](https://binghao-huang.github.io/vt_refine/)


## ğŸ·ï¸Â License

This project is released under the **MIT License**. See [LICENSE](LICENSE) for details.


## ğŸ™Â Acknowledgements

Our Visuo-Tactile Gripper builds upon [UMI Gripper](https://umi-gripper.github.io/umi.pdf). The SLAM pipeline builds upon [SteffenÂ Urban](https://github.com/urbste)â€™s fork of [ORB\_SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3) and his [OpenImuCameraCalibrator](https://github.com/urbste/OpenImuCameraCalibrator/).

The gripperâ€™s mechanical design is adapted from the [Push/Pull Gripper](https://www.thingiverse.com/thing:2204113) by [JohnÂ Mulac](https://www.thingiverse.com/3dprintingworld/designs), and the soft finger from an original design by [AlexÂ Alspach](http://alexalspach.com/) at TRI. The GoPro installation frame on robot side is adapted from [Fast-UMI](https://arxiv.org/abs/2409.19499).


